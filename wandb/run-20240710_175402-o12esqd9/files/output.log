==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
Epoch: 0
/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(



















 [============================ 20/20 ==========================>...]  Step: 2s718ms | Tot: 2m12s | Train Loss: 2.334 | Train Acc: 11.260% (1126/10000)












 [============================ 100/100 ===========================>]  Step: 230ms | Tot: 25s309ms | Test Loss: 2.149 | Test Acc: 20.060% (2006/10000)
/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Saving..
bin_lower=0.100000, bin_upper=0.200000, accuracy=0.1615, confidence=0.1572:
bin_lower=0.200000, bin_upper=0.300000, accuracy=0.3541, confidence=0.2265:
bin_lower=0.300000, bin_upper=0.400000, accuracy=1.0000, confidence=0.3039:
ece =  tensor([0.0294])
Wed Jul 10 17:57:12 2024 Epoch 0, lr: 0.0001000, val loss: 214.88781, acc: 20.06000
[214.88780784606934]
Epoch: 1



















 [============================ 20/20 ==========================>...]  Step: 2s950ms | Tot: 2m10s | Train Loss: 2.289 | Train Acc: 12.840% (1284/10000)














 [============================ 100/100 ===========================>]  Step: 247ms | Tot: 26s416ms | Test Loss: 2.072 | Test Acc: 23.240% (2324/10000)
Saving..
bin_lower=0.100000, bin_upper=0.200000, accuracy=0.1951, confidence=0.1632:
bin_lower=0.200000, bin_upper=0.300000, accuracy=0.3412, confidence=0.2256:
bin_lower=0.300000, bin_upper=0.400000, accuracy=0.5000, confidence=0.3101:
ece =  tensor([0.0533])
Wed Jul 10 18:00:19 2024 Epoch 1, lr: 0.0001000, val loss: 207.18361, acc: 23.24000
[214.88780784606934, 207.18361139297485]
Epoch: 2



















 [============================ 20/20 ==========================>...]  Step: 2s843ms | Tot: 2m14s | Train Loss: 2.243 | Train Acc: 15.650% (1565/10000)













 [============================ 100/100 ===========================>]  Step: 379ms | Tot: 26s763ms | Test Loss: 1.963 | Test Acc: 29.020% (2902/10000)
Saving..
bin_lower=0.100000, bin_upper=0.200000, accuracy=0.2228, confidence=0.1707:
bin_lower=0.200000, bin_upper=0.300000, accuracy=0.3528, confidence=0.2364:
bin_lower=0.300000, bin_upper=0.400000, accuracy=0.4436, confidence=0.3342:
bin_lower=0.400000, bin_upper=0.500000, accuracy=0.5361, confidence=0.4315:
bin_lower=0.500000, bin_upper=0.600000, accuracy=1.0000, confidence=0.5258:
ece =  tensor([0.0810])
Wed Jul 10 18:03:31 2024 Epoch 2, lr: 0.0001000, val loss: 196.29942, acc: 29.02000
[214.88780784606934, 207.18361139297485, 196.2994179725647]
Epoch: 3




















 [============================ 20/20 ==========================>...]  Step: 2s801ms | Tot: 2m10s | Train Loss: 2.191 | Train Acc: 18.090% (1809/10000)













 [============================ 100/100 ===========================>]  Step: 291ms | Tot: 26s192ms | Test Loss: 1.865 | Test Acc: 31.150% (3115/10000)
Saving..
bin_lower=0.100000, bin_upper=0.200000, accuracy=0.2007, confidence=0.1740:
bin_lower=0.200000, bin_upper=0.300000, accuracy=0.3118, confidence=0.2450:
bin_lower=0.300000, bin_upper=0.400000, accuracy=0.3939, confidence=0.3410:
bin_lower=0.400000, bin_upper=0.500000, accuracy=0.5094, confidence=0.4355:
bin_lower=0.500000, bin_upper=0.600000, accuracy=0.4714, confidence=0.5298:
bin_lower=0.600000, bin_upper=0.700000, accuracy=0.0000, confidence=0.6084:
ece =  tensor([0.0545])
Wed Jul 10 18:06:38 2024 Epoch 3, lr: 0.0001000, val loss: 186.53078, acc: 31.15000
[214.88780784606934, 207.18361139297485, 196.2994179725647, 186.5307832956314]
Epoch: 4



















 [============================ 20/20 ==========================>...]  Step: 2s870ms | Tot: 2m10s | Train Loss: 2.149 | Train Acc: 19.280% (1928/10000)
















 [============================ 100/100 ===========================>]  Step: 247ms | Tot: 31s49ms | Test Loss: 1.804 | Test Acc: 34.860% (3486/10000)
Saving..
bin_lower=0.100000, bin_upper=0.200000, accuracy=0.1742, confidence=0.1775:
bin_lower=0.200000, bin_upper=0.300000, accuracy=0.3146, confidence=0.2490:
bin_lower=0.300000, bin_upper=0.400000, accuracy=0.4373, confidence=0.3399:
bin_lower=0.400000, bin_upper=0.500000, accuracy=0.5827, confidence=0.4380:
bin_lower=0.500000, bin_upper=0.600000, accuracy=0.6847, confidence=0.5289:
ece =  tensor([0.0715])
Wed Jul 10 18:09:50 2024 Epoch 4, lr: 0.0001000, val loss: 180.41559, acc: 34.86000
[214.88780784606934, 207.18361139297485, 196.2994179725647, 186.5307832956314, 180.415585398674]
Epoch: 5



















 [============================ 20/20 ==========================>...]  Step: 2s958ms | Tot: 2m7s | Train Loss: 2.119 | Train Acc: 20.340% (2034/10000)




 [==================>......... 29/100 .............................]  Step: 274ms | Tot: 7s496ms | Test Loss: 1.742 | Test Acc: 38.448% (1115/2900)
Traceback (most recent call last):
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/train_cifar10_cpu.py", line 423, in <module>
    # running on cpu doesn't allow multiprocessing
        ^^^^^^
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/train_cifar10_cpu.py", line 393, in main
    start = time.time()
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/train_cifar10_cpu.py", line 342, in test
    inputs, targets = inputs.to(device), targets.to(device)
              ^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/models/vit_small.py", line 139, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/models/vit_small.py", line 82, in forward
    x = ff(x) + x
        ^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/models/vit_small.py", line 24, in forward
    return self.fn(self.norm(x), **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/models/vit_small.py", line 37, in forward
    return self.net(x)
           ^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt