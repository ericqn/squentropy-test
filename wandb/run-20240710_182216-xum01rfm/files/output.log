==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
Epoch: 0
/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(



















 [============================ 20/20 ==========================>...]  Step: 2s813ms | Tot: 2m5s | Train Loss: 2.329 | Train Acc: 11.840% (1184/10000)














 [============================ 100/100 ===========================>]  Step: 261ms | Tot: 26s253ms | Test Loss: 2.173 | Test Acc: 18.620% (1862/10000)
Saving..
ece =  tensor([0.0259])
Wed Jul 10 18:25:22 2024 Epoch 0, lr: 0.0001000, val loss: 217.31235, acc: 18.62000
[2.173123505115509]
Epoch: 1
/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)



















 [============================ 20/20 ==========================>...]  Step: 2s718ms | Tot: 2m8s | Train Loss: 2.291 | Train Acc: 13.190% (1319/10000)













 [============================ 100/100 ===========================>]  Step: 352ms | Tot: 27s289ms | Test Loss: 2.083 | Test Acc: 25.320% (2532/10000)
Saving..
ece =  tensor([0.0655])
Wed Jul 10 18:28:28 2024 Epoch 1, lr: 0.0001000, val loss: 208.29877, acc: 25.32000
[2.173123505115509, 2.0829877138137816]
Epoch: 2















 [============================ 16/20 =============>................]  Step: 6s584ms | Tot: 1m44s | Train Loss: 1.807 | Train Acc: 15.649% (1282/8192)
Traceback (most recent call last):
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/train_cifar10_cpu.py", line 425, in <module>
    main()
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/train_cifar10_cpu.py", line 394, in main
    trainloss = train(epoch)
                ^^^^^^^^^^^^
  File "/Users/ericnguyen/Belkin Lab/vision-transformers-cifar10/train_cifar10_cpu.py", line 314, in train
    loss.backward()
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ericnguyen/anaconda3/envs/belkin/lib/python3.12/site-packages/wandb/wandb_torch.py", line 272, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt